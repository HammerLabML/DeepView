{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b2d602-bca0-4716-8759-433c8cd47bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T08:54:54.210773823Z",
     "start_time": "2024-03-27T08:54:54.197422607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor: Isaac Roberts (Jay)\\nEmployer: University of Bielefeld\\nResearch Group: Machine Learning Group (HammerLab)\\n\\n\\nOverview: This notebook contains the code to conduct the Local Neighborhood Analysis as seen in {}. \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Isaac Roberts (Jay)\n",
    "Employer: University of Bielefeld\n",
    "Research Group: Machine Learning Group (HammerLab)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Overview: This notebook contains the code to conduct the Local Neighborhood Analysis as seen in Section 5.4 \n",
    "          of the \"Targeted Visualization of the Backbone of Encoder LLMs\" paper. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861a94f7-bf6a-42e6-ae6f-449668689848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 09:56:42.330620: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 09:56:42.459888: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-27 09:56:42.464765: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:56:42.464777: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-27 09:56:42.490391: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-27 09:56:43.068177: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:56:43.068233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:56:43.068239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/iroberts/miniconda3/envs/ijcnn/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/iroberts/miniconda3/envs/ijcnn/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/iroberts/miniconda3/envs/ijcnn/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/iroberts/miniconda3/envs/ijcnn/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Import functions from Multi_task model code files\n",
    "from NLP_helper_scripts.Multi_Task_Data_load import *\n",
    "from NLP_helper_scripts.Multi_Task_model import *\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# multi task model arguments \n",
    "model_args = ModelArguments(model_name_or_path=\"bert-base-uncased\")\n",
    "\n",
    "# multi task training arguments \n",
    "training_args = TrainingArguments(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir=\"./models/mrpc_cola_rte/5epoch_bert\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# multi task model tokenizer \n",
    "multi_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "#single task model tokenizer\n",
    "\n",
    "# we should make sure that the tokenizers are the same. Results do not very if we do use another\n",
    "# tokenizer but they can vary slightly\n",
    "single_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "#use this to dictate which Glue Task to examine for the single task model\n",
    "task = \"sst2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7572d13-aac4-4ae6-959b-6f635f52f13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), [])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors.torch import load_model\n",
    "\n",
    "# GPU device \n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# list of tasks included in the multi task model\n",
    "multi_tasks = [Task(id=0, name='cola', type='glue', num_labels=2),\n",
    " Task(id=1, name='mnli', type='glue', num_labels=3),\n",
    " Task(id=2, name='mrpc', type='glue', num_labels=2),\n",
    " Task(id=3, name='qnli', type='glue', num_labels=2),\n",
    " Task(id=4, name='qqp', type='glue', num_labels=2),\n",
    " Task(id=5, name='rte', type='glue', num_labels=2),\n",
    " Task(id=6, name='sst2', type='glue', num_labels=2)]\n",
    "\n",
    "\n",
    "# Calling the class of the multi task model and putting it on our GPU device selected above\n",
    "multi_model = MultiTaskModel(model_args.model_name_or_path, tasks=multi_tasks).to(device)\n",
    "\n",
    "# Grabs the weights from the checkpoint you select\n",
    "load_model(multi_model,\"./models/multi-task-model/checkpoint-350000/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3a51c5-f451-4127-befc-0a2b49529b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the data from HF into memory \n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\"else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load(\"glue\", actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1cd867-2dbe-4460-a54c-42165f161e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: hide new secretions from the parental units \n"
     ]
    }
   ],
   "source": [
    "#dictionary of fields by Glue task\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "#filtering the fields to the task selected above\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "\n",
    "#test by printing to make sure you have selected the right dataset\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")\n",
    "\n",
    "\n",
    "# functions to tokenize the inputs\n",
    "\n",
    "#multi task model tokenize\n",
    "def multi_preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "       return  multi_tokenizer(examples[sentence1_key], max_length=128,padding='max_length',truncation=True)\n",
    "    return multi_tokenizer(examples[sentence1_key], examples[sentence2_key], max_length=128,padding='max_length',truncation=True)\n",
    "\n",
    "\n",
    "# single task model tokenize\n",
    "def single_preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "       return  single_tokenizer(examples[sentence1_key], max_length=128,padding='max_length',truncation=True)\n",
    "    return single_tokenizer(examples[sentence1_key], examples[sentence2_key], max_length=128,padding='max_length',truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65a7d91-9d23-4b0e-9a53-2e3681d32d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the functions to each portion of the datasets: train, validation\n",
    "multi_preprocessed_dataset = dataset.map(multi_preprocess_function, batched=True)\n",
    "single_preprocessed_dataset = dataset.map(single_preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dbfc20a-536f-4bd5-b396-f78474ff0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLP_helper_scripts.getData import prepare_label\n",
    "\n",
    "validation_key = (\n",
    "    \"validation_mismatched\"\n",
    "    if task == \"mnli-mm\"\n",
    "    else \"validation_matched\"\n",
    "    if task == \"mnli\"\n",
    "    else \"validation\"\n",
    ")\n",
    "\n",
    "\n",
    "#encoding the data into numpy\n",
    "def bert_glue_encode(dataset):\n",
    "    # Convert batch of encoded features to numpy array.\n",
    "    input_ids = np.array(dataset[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(dataset[\"attention_mask\"], dtype=\"int32\")\n",
    "    token_type_ids = np.array(dataset[\"token_type_ids\"], dtype=\"int32\")\n",
    "    labels = np.array(dataset[\"label\"], dtype=\"int32\")\n",
    "\n",
    "    #add check for test set since they may not have labels\n",
    "    return (input_ids, attention_masks, token_type_ids) ,labels\n",
    "\n",
    "m_x_val, y_val = bert_glue_encode(multi_preprocessed_dataset[validation_key])\n",
    "x_val, y_val = bert_glue_encode(single_preprocessed_dataset[validation_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3928e42-757c-4e88-951d-de144046425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#setting the number of samples to examine\n",
    "n_samples = 500\n",
    "\n",
    "#random sample of indexes which we will use to filter\n",
    "sample_ids = np.random.choice(len(x_val[1]), n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e77abc12-0bf5-4ce7-b9e5-6fe7b86c93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our gpu does not contain a lot of memory we need to split the larger sample sizes up in order to embed\n",
    "# the inputs with the multi_task model\n",
    "\n",
    "import gc\n",
    "\n",
    "# make sure that the num_splits divides the n_samples evenly\n",
    "num_splits = 5\n",
    "multi_sample = []\n",
    "\n",
    "#this number should match the task head number\n",
    "task = [ 4 for i in range(len(y_val))]\n",
    "\n",
    "# this code splits the sample\n",
    "for i in range(num_splits):\n",
    "    if i != (len(range(num_splits)) - 1):\n",
    "        multi_input_ids = torch.LongTensor(m_x_val[0][sample_ids][(i*int(n_samples/num_splits)):(i+1)* int(n_samples/num_splits)]).to(device)\n",
    "        multi_attention_mask = torch.LongTensor(m_x_val[1][sample_ids][(i*int(n_samples/num_splits)):(i+1)* int(n_samples/num_splits)]).to(device)\n",
    "        multi_token_type_ids = torch.LongTensor(m_x_val[2][sample_ids][(i*int(n_samples/num_splits)):(i+1)* int(n_samples/num_splits)]).to(device)\n",
    "        multi_task_ids = torch.LongTensor(task[(i*int(n_samples/num_splits)):(i+1)* int(n_samples/num_splits)]).to(device)\n",
    "    else: \n",
    "        multi_input_ids = torch.LongTensor(m_x_val[0][sample_ids][(i*int(n_samples/num_splits)):int(n_samples)]).to(device)\n",
    "        multi_attention_mask = torch.LongTensor(m_x_val[1][sample_ids][(i*int(n_samples/num_splits)):int(n_samples)]).to(device)\n",
    "        multi_token_type_ids = torch.LongTensor(m_x_val[2][sample_ids][(i*int(n_samples/num_splits)):int(n_samples)]).to(device)\n",
    "        multi_task_ids = torch.LongTensor(task[(i*int(n_samples/num_splits)):int(n_samples)]).to(device)\n",
    "\n",
    "    #embed the split with the multi task model\n",
    "    multi_sample_embedding = multi_model.embed(multi_input_ids, multi_attention_mask, multi_token_type_ids, task_ids=multi_task_ids)\n",
    "\n",
    "    #save to memory\n",
    "    multi_sample_np = multi_sample_embedding.detach().cpu().numpy()\n",
    "\n",
    "    #append to the list defined above\n",
    "    multi_sample.append(multi_sample_np)\n",
    "\n",
    "\n",
    "    #deletes the embedded sample from memeory to free it up \n",
    "    del multi_sample_embedding\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#rearrange the sample into its proper dimensions\n",
    "multi_sample = np.array(multi_sample).reshape((n_samples,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c702dc-5e39-48fe-8ded-7965e84eafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the multi task classification head to use for Deepview\n",
    "def pred_wrapper_multitask(x):\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    with torch.no_grad():\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        tensor = torch.from_numpy(x).to(device)\n",
    "\n",
    "        # 4 is the id of QQP in the list given to the multi task model\n",
    "        logits, loss = multi_model.output_heads[str(4)](tensor)\n",
    "        probabilities = softmax(logits).detach().cpu().numpy()\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "031d3dde-7fbb-44b6-93e9-d01e18ca7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_Y1 = y_val[sample_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "344bca16-4977-40f4-aa5f-0f58ef146fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 09:58:21.865324: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:58:21.865499: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:58:21.865613: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:58:21.865717: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:58:21.865888: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:58:21.865998: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-03-27 09:58:21.866076: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-03-27 09:58:21.866624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 43s 3s/step\n"
     ]
    }
   ],
   "source": [
    "#this codes loads the pretrained classifer and Bert models\n",
    "from NLP_helper_scripts.getData import train_val_test_split, pretrained_bert_model, prepare_dataset, classifier_model, finetuned_bert_and_classifier\n",
    "import tensorflow as tf\n",
    "\n",
    "#path to model\n",
    "saved_pretrained_classifier_only_path = './models/' + str(task)+ '/{}_pretrained_BERT_Classifier'.format(task.replace('/', '_'))\n",
    "\n",
    "#loads a pre-trained bert model\n",
    "pretrained_bert = pretrained_bert_model()\n",
    "\n",
    "#loads the pre-trained model\n",
    "pretrained_classifier = tf.saved_model.load(saved_pretrained_classifier_only_path)\n",
    "\n",
    "#embeds the validation set using the pre-trained Bert model\n",
    "val_embeddings  = pretrained_bert.predict(x_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c2ec7ad-493a-4782-9e48-ead508976b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the DeepView prediction wraper for the classifier\n",
    "infer = pretrained_classifier.signatures['serving_default']\n",
    "def pretrained_pred_wrapper(x):\n",
    "    to_tensor = x.reshape((1,len(x),768))\n",
    "    tensor = tf.constant(to_tensor, dtype = 'float32')\n",
    "    init_preds = infer(tensor)\n",
    "    init_preds = init_preds['output'].numpy()\n",
    "\n",
    "    #make sure to check the number of classes in this case there are only 2\n",
    "    preds = init_preds.reshape((len(x),2))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bd93b-e907-4add-8db4-1a45a6165340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepview import DeepView\n",
    "\n",
    "# grabs the sample ideas from the val;idation set for the Pretrained Deepview\n",
    "pre_X1 = np.array([ val_embeddings[i] for i in sample_ids ])\n",
    "pre_Y1 = np.array([ y_val[i] for i in sample_ids ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54080ab8-0563-4e96-a64b-80c7ac72d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tf_bert_model_1': TensorSpec(shape=(None, 768), dtype=tf.float32, name='tf_bert_model_1')}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as text\n",
    "\n",
    "#loads the fine-tuned models\n",
    "saved_finetuned_whole_model_path = './models/' + str(task)+ '/{}_finetuned_BERT'.format(task.replace('/', '_'))\n",
    "saved_finetuned_embed_model_path = './models/' + str(task)+ '/{}_finetuned_BERT_Embeddings'.format(task.replace('/', '_'))\n",
    "saved_finetuned_predict_model_path = './models/' + str(task)+ '/{}_finetuned_BERT_Predictor'.format(task.replace('/', '_'))\n",
    "\n",
    "bert_embed = tf.saved_model.load(saved_finetuned_embed_model_path)\n",
    "predict_model = tf.saved_model.load(saved_finetuned_predict_model_path)\n",
    "whole_model = tf.saved_model.load(saved_finetuned_whole_model_path)\n",
    "infer1 = bert_embed.signatures[\"serving_default\"]\n",
    "print(infer1.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbace284-e651-4a4a-869f-0c49b8c79bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes the samples and embeds them with the fine-tuned model\n",
    "inputs = x_val[0][sample_ids]\n",
    "attention = x_val[1][sample_ids]\n",
    "token_type = x_val[2][sample_ids]\n",
    "\n",
    "t = (inputs, attention, token_type)\n",
    "samples = tf.constant(t)\n",
    "\n",
    "test_embedding  = infer1(attention_masks=samples[1], input_ids=samples[0], token_type_ids=samples[2])\n",
    "\n",
    "test_embedding = test_embedding['tf_bert_model_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c45a660b-e14c-4154-9092-2f8804580adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the sample for the finetuned model\n",
    "fine_X1 = test_embedding.numpy()\n",
    "fine_Y1 = np.array([ y_val[i] for i in sample_ids ])\n",
    "\n",
    "# create a wrapper function for deepview\n",
    "# need further preprocessing or conversion into a tensor datatype\n",
    "finetuned_pred_wrapper = DeepView.create_simple_wrapper(predict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc883d-7c32-43c0-9b47-f3ef1ae64310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--- Deep View Parameters ----\n",
    "use_case = \"nlp\"\n",
    "# tasks = torch.unique(single_task_ids).detach().cpu().numpy()\n",
    "classes = np.arange(2)\n",
    "batch_size = 64\n",
    "max_samples = 500\n",
    "data_shape = (768,)\n",
    "resolution = 100\n",
    "N = 10\n",
    "lam = 1\n",
    "cmap = 'tab10'\n",
    "metric = 'cosine'\n",
    "disc_dist = (\n",
    "    False\n",
    "    if lam == 1\n",
    "    else True\n",
    ")\n",
    "\n",
    "# to make sure deepview.show is blocking,\n",
    "# disable interactive mode\n",
    "interactive = False\n",
    "title = 'movie-reviews BERT'\n",
    "\n",
    "\n",
    "multi_og_deepview_task_cosine = DeepView(pred_wrapper_multitask, classes, max_samples, batch_size, data_shape,\n",
    "                      N, lam, resolution, cmap, interactive, title, metric=metric, disc_dist=disc_dist)\n",
    "\n",
    "multi_og_deepview_task_cosine.add_samples(multi_sample,multi_Y1)\n",
    "\n",
    "fine_og_deepview_task_cosine = DeepView(finetuned_pred_wrapper, classes, max_samples, batch_size, data_shape,\n",
    "                      N, lam, resolution, cmap, interactive, title, metric=metric, disc_dist=disc_dist)\n",
    "\n",
    "fine_og_deepview_task_cosine.add_samples(fine_X1,fine_Y1)\n",
    "\n",
    "pre_og_deepview_task_cosine = DeepView(pretrained_pred_wrapper, classes, max_samples, batch_size, data_shape,\n",
    "                      N, lam, resolution, cmap, interactive, title, metric=metric, disc_dist=disc_dist)\n",
    "\n",
    "pre_og_deepview_task_cosine.add_samples(pre_X1,pre_Y1)\n",
    "\n",
    "pre_og_deepview_task_half = DeepView(pretrained_pred_wrapper, classes, max_samples, batch_size, data_shape,\n",
    "                      N, .8, resolution, cmap, interactive, title, metric=metric, disc_dist=True)\n",
    "\n",
    "pre_og_deepview_task_half.add_samples(pre_X1,pre_Y1)\n",
    "\n",
    "pre_og_deepview_task_fisher = DeepView(pretrained_pred_wrapper, classes, max_samples, batch_size, data_shape,\n",
    "                      N, 0, resolution, cmap, interactive, title, metric=metric, disc_dist=True)\n",
    "\n",
    "pre_og_deepview_task_fisher.add_samples(pre_X1,pre_Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a941c3-c0e3-4ffa-8504-7cf96869e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_og_deepview_task_cosine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93b6e6-c861-4b15-ad69-6e708a2eb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_og_deepview_task_cosine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56866e38-25f5-4fce-aefe-7360b5882d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_og_deepview_task_cosine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149509e1-3802-4e64-a2d8-be719a6bffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_og_deepview_task_half.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6954d-5047-49ed-82ff-434e38bbd2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_og_deepview_task_fisher.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93947c1f-1fce-4ebe-ba77-22d02e5d8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def count_distance_neighbors(og_dist,og_preds, pert_dist, pert_preds, k):\n",
    "    '''\n",
    "    function that takes deepview distance and computes a Q_NN(k) value\n",
    "    '''\n",
    "    pert_nn = KNeighborsClassifier(n_neighbors=k, metric=\"precomputed\")\n",
    "    pert_nn.fit(np.round(pert_dist,7), pert_preds)\n",
    "\n",
    "    og_nn = KNeighborsClassifier(n_neighbors=k, metric=\"precomputed\")\n",
    "    og_nn.fit(np.round(og_dist,7), og_preds) \n",
    "\n",
    "    og_neighs = og_nn.kneighbors(return_distance=False)\n",
    "    pert_neighs = pert_nn.kneighbors(return_distance=False)\n",
    "\n",
    "    counts = 0\n",
    "    for base,pert in zip(og_neighs,pert_neighs):\n",
    "        for i in base:\n",
    "            if i in pert:\n",
    "                counts += (1/k)\n",
    "    \n",
    "    return counts/len(og_dist)\n",
    "\n",
    "def qnn_k(og_dist, og_preds, pert_dist, pert_preds, k):\n",
    "    '''\n",
    "    function that takes deepview embedding and computes a Q_NN(k) value\n",
    "    '''\n",
    "    pert_nn = KNeighborsClassifier(n_neighbors=k)\n",
    "    pert_nn.fit(np.round(pert_dist,7), pert_preds)\n",
    "\n",
    "    og_nn = KNeighborsClassifier(n_neighbors=k)\n",
    "    og_nn.fit(np.round(og_dist,7), og_preds) \n",
    "\n",
    "    og_neighs = og_nn.kneighbors(return_distance=False)\n",
    "    pert_neighs = pert_nn.kneighbors(return_distance=False)\n",
    "\n",
    "    global_counts = 0\n",
    "    for base,pert in zip(og_neighs,pert_neighs):\n",
    "        for i in base:\n",
    "            if i in pert:\n",
    "                global_counts += 1\n",
    "\n",
    "    return global_counts/(len(og_dist) * k)\n",
    "\n",
    "\n",
    "\n",
    "def qnn_k_distribution(og_dist, og_preds, pert_dist, pert_preds, k):\n",
    "     '''\n",
    "    function that takes deepview embedding and computes the distribution for a particular k\n",
    "    '''\n",
    "    pert_nn = KNeighborsClassifier(n_neighbors=k)\n",
    "    pert_nn.fit(np.round(pert_dist,7), pert_preds)\n",
    "\n",
    "    og_nn = KNeighborsClassifier(n_neighbors=k)\n",
    "    og_nn.fit(np.round(og_dist,7), og_preds) \n",
    "\n",
    "    og_neighs = og_nn.kneighbors(return_distance=False)\n",
    "    pert_neighs = pert_nn.kneighbors(return_distance=False)\n",
    "\n",
    "    local_counts = []\n",
    "    local_count = 0 \n",
    "    for base,pert in zip(og_neighs,pert_neighs):\n",
    "        for i in base:\n",
    "            if i in pert:\n",
    "                local_count += 1\n",
    "        \n",
    "        local_counts.append(local_count)\n",
    "        local_count = 0\n",
    "    \n",
    "    return np.array(local_counts)\n",
    "\n",
    "def lcmc(qnn_list,ks):\n",
    "     '''\n",
    "    function that task a list of Q_NN(k) values and normalizaes it for small k's producing a Q_local score\n",
    "    '''\n",
    "    lcmcs = []\n",
    "    for i in ks_:\n",
    "        lcmc = qnn_list[i-1] - (i/(len(ks)-1))\n",
    "        lcmcs.append(lcmc)\n",
    "\n",
    "    return lcmcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb8f8b5f-01de-4354-8f45-2e9e16bd9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all possible k values\n",
    "ks_ = list(range(1,500))\n",
    "\n",
    "\n",
    "#we use a list comprehension to get a list of Q_NN(k) values over all possible k's\n",
    "multi_fine_cosine_avg_count = [qnn_k(multi_og_deepview_task_cosine.embedded,multi_og_deepview_task_cosine.y_pred,\n",
    "                                        fine_og_deepview_task_cosine.embedded,fine_og_deepview_task_cosine.y_pred,k) for k in ks_]\n",
    "\n",
    "fine_pre_cosine_avg_count = [qnn_k(fine_og_deepview_task_cosine.embedded,fine_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_cosine.embedded,pre_og_deepview_task_cosine.y_pred,k) for k in ks_]\n",
    "\n",
    "multi_pre_cosine_avg_count = [qnn_k(multi_og_deepview_task_cosine.embedded,multi_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_cosine.embedded,pre_og_deepview_task_cosine.y_pred,k) for k in ks_]\n",
    "\n",
    "fine_prehalf_cosine_avg_count = [qnn_k(fine_og_deepview_task_cosine.embedded,fine_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_half.embedded,pre_og_deepview_task_half.y_pred,k) for k in ks_]\n",
    "\n",
    "multi_prehalf_cosine_avg_count = [qnn_k(multi_og_deepview_task_cosine.embedded,multi_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_half.embedded,pre_og_deepview_task_half.y_pred,k) for k in ks_]\n",
    "\n",
    "fine_prefisher_cosine_avg_count = [qnn_k(fine_og_deepview_task_cosine.embedded,fine_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_fisher.embedded,pre_og_deepview_task_fisher.y_pred,k) for k in ks_]\n",
    "\n",
    "multi_prefisher_cosine_avg_count = [qnn_k(multi_og_deepview_task_cosine.embedded,multi_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_fisher.embedded,pre_og_deepview_task_fisher.y_pred,k) for k in ks_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b55b4-1e3d-4d5a-9020-178ea76181aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code creates a Q_NN(k) curve and LCMC curve plot\n",
    "\n",
    "lcmcs = []\n",
    "auc_scores = []\n",
    "q_locals = []\n",
    "k_maxes = []\n",
    "qnn_list = [multi_fine_cosine_avg_count,fine_pre_cosine_avg_count,multi_pre_cosine_avg_count,fine_prehalf_cosine_avg_count,\n",
    "            multi_prehalf_cosine_avg_count,fine_prefisher_cosine_avg_count,multi_prefisher_cosine_avg_count]\n",
    "\n",
    "figure, ax = plt.subplots(1, 2, figsize=(10, 4) )\n",
    "legend_list = ['MT vs FT','FT vs PT (\\u03BB = 1)','MT vs PT (\\u03BB = 1)','FT vs PT (\\u03BB = 0.8)',\n",
    "               'MT vs PT (\\u03BB = 0.8)','FT vs PT (\\u03BB = 0.0)','MT vs PT (\\u03BB = 0.0)']\n",
    "\n",
    "i = 0\n",
    "for qnn in qnn_list:\n",
    "    lc = lcmc(qnn, ks_)\n",
    "\n",
    "\n",
    "    qnn_array = np.array(qnn)\n",
    "    lcmcs_array = np.array(lc)\n",
    "    max_arg = np.argmax(lc)\n",
    "    q_local = np.sum(qnn_array[0:max_arg])/ ks_[max_arg]\n",
    "\n",
    "\n",
    "    lcmcs.append(lc)\n",
    "    auc_scores.append(sum(qnn)/len(ks_))\n",
    "    q_locals.append(q_local)\n",
    "    k_maxes.append(ks_[max_arg])\n",
    "\n",
    "\n",
    "    ax[0].plot(ks_, qnn, label = legend_list[i])\n",
    "    ax[1].plot(ks_, lc, label = legend_list[i])\n",
    "    \n",
    "    ax[1].legend(loc=\"upper right\",prop={'size': 8})\n",
    "    # Set common labels\n",
    "    ax[0].set_xlabel('K')\n",
    "    ax[0].set_ylabel('Q_NN(k) Score')\n",
    "    ax[1].set_xlabel('K')\n",
    "    ax[1].set_ylabel('LCMC')\n",
    "    ax[0].set_title('Co-Nearest Neighbor Chart (Q_NN(k))' )\n",
    "    ax[1].set_title(\"Co-Nearest Neighbor Normalized for Small K's\")\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e80255-aa27-4ab6-8c84-7d346f275ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_locals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266de37e-de43-470d-8b00-af8707d12cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b23cb8-e938-4fd7-b8d3-fdc79cadebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973a759-c1b6-4737-8748-aac8fc7001e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_fine_cosine_avg_count_high = [count_distance_neighbors_high(multi_og_deepview_task_cosine.distances,multi_og_deepview_task_cosine.y_pred,\n",
    "                                        fine_og_deepview_task_cosine.distances,fine_og_deepview_task_cosine.y_pred,k) for k in ks_]\n",
    "\n",
    "fine_pre_cosine_avg_count_high = [count_distance_neighbors_high(fine_og_deepview_task_cosine.distances,fine_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_cosine.distances,pre_og_deepview_task_cosine.y_pred,k) for k in ks_]\n",
    "\n",
    "multi_pre_cosine_avg_count_high = [count_distance_neighbors_high(multi_og_deepview_task_cosine.distances,multi_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_cosine.distances,pre_og_deepview_task_cosine.y_pred,k) for k in ks_]\n",
    "\n",
    "fine_prehalf_cosine_avg_count_high = [count_distance_neighbors_high(fine_og_deepview_task_cosine.distances,fine_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_half.distances,pre_og_deepview_task_half.y_pred,k) for k in ks_]\n",
    "\n",
    "multi_prehalf_cosine_avg_count_high = [count_distance_neighbors_high(multi_og_deepview_task_cosine.distances,multi_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_half.distances,pre_og_deepview_task_half.y_pred,k) for k in ks_]\n",
    "\n",
    "fine_prefisher_cosine_avg_count_high = [count_distance_neighbors_high(fine_og_deepview_task_cosine.distances,fine_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_fisher.distances,pre_og_deepview_task_fisher.y_pred,k) for k in ks_]\n",
    "\n",
    "multi_prefisher_cosine_avg_count_high = [count_distance_neighbors_high(multi_og_deepview_task_cosine.distances,multi_og_deepview_task_cosine.y_pred,\n",
    "                                        pre_og_deepview_task_fisher.distances,pre_og_deepview_task_fisher.y_pred,k) for k in ks_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af283f-8942-458f-8840-64072deea983",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcmcs = []\n",
    "auc_scores = []\n",
    "q_locals = []\n",
    "k_maxes = []\n",
    "qnn_list = [multi_fine_cosine_avg_count_high,fine_pre_cosine_avg_count_high,multi_pre_cosine_avg_count_high,\n",
    "            fine_prehalf_cosine_avg_count_high,\n",
    "            multi_prehalf_cosine_avg_count_high,fine_prefisher_cosine_avg_count_high,multi_prefisher_cosine_avg_count_high]\n",
    "\n",
    "figure, ax = plt.subplots(1, 2, figsize=(10, 4) )\n",
    "legend_list = ['MT vs FT','FT vs PT (\\u03BB = 1)','MT vs PT (\\u03BB = 1)','FT vs PT (\\u03BB = 0.8)',\n",
    "               'MT vs PT (\\u03BB = 0.8)','FT vs PT (\\u03BB = 0.0)','MT vs PT (\\u03BB = 0.0)']\n",
    "\n",
    "i = 0\n",
    "for qnn in qnn_list:\n",
    "    lc = lcmc(qnn, ks_)\n",
    "\n",
    "\n",
    "    qnn_array = np.array(qnn)\n",
    "    lcmcs_array = np.array(lc)\n",
    "    max_arg = np.argmax(lc)\n",
    "    q_local = np.sum(qnn_array[0:max_arg])/ ks_[max_arg]\n",
    "\n",
    "\n",
    "    lcmcs.append(lc)\n",
    "    auc_scores.append(sum(qnn)/len(ks_))\n",
    "    q_locals.append(q_local)\n",
    "    k_maxes.append(ks_[max_arg])\n",
    "\n",
    "\n",
    "    ax[0].plot(ks_, qnn, label = legend_list[i])\n",
    "    ax[1].plot(ks_, lc, label = legend_list[i])\n",
    "    \n",
    "    ax[1].legend(loc=\"upper right\",prop={'size': 8})\n",
    "    # Set common labels\n",
    "    ax[0].set_xlabel('K')\n",
    "    ax[0].set_ylabel('Q_NN(k) Score')\n",
    "    ax[1].set_xlabel('K')\n",
    "    ax[1].set_ylabel('LCMC')\n",
    "    ax[0].set_title('Co-Nearest Neighbor Chart (Q_NN(k))' )\n",
    "    ax[1].set_title(\"Co-Nearest Neighbor Normalized for Small K's\")\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524c7f8-d44f-44d4-83ce-1e29e8d6ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = qnn_k_distribution(multi_og_deepview_task_cosine.embedded, multi_og_deepview_task_cosine.y_pred,\n",
    "                                        fine_og_deepview_task_cosine.embedded,fine_og_deepview_task_cosine.y_pred,100)\n",
    "\n",
    "plt.hist(dist, bins = list(range(0,100)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe3b85-9c34-499e-a3d4-e5c26f039694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
