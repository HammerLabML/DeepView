{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepview import DeepView\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# ---------------------------\n",
    "import demo_utils as demo\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib qt seems to be a bit buggy with notebooks, so we execute it multiple times\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"+1\"><b>\n",
    "    \n",
    " 0. [Usage Instructions](#DeepView-Usage-Instructions)\n",
    " 0. [Setting up DeepView](#Demo-with-Torch-model)\n",
    " 0. [Tuning $\\lambda$ Hyperparameter](#Tuning-the-$\\lambda$-Hyperparameter)\n",
    " \n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data and models\n",
    " \n",
    "Each section in this notebook can be run independently, thus at the beginning of each section, the according model (i.e. torch/knn/decision tree) and the dataset will be initialized. The reason for this is, that running both torch and tensorflow simultaneously on the GPU may lead to problems.\n",
    "This notebook tests the DeepView framework on different classifiers\n",
    "\n",
    " * ResNet-20 on CIFAR10\n",
    " * DecisionTree on MNIST\n",
    " * RandomForest on MNIST\n",
    " * KNN on MNIST\n",
    "\n",
    "---\n",
    "\n",
    "## DeepView Usage Instructions\n",
    "\n",
    " 1. Create a wrapper funktion like ```pred_wrapper``` which receives a numpy array of samples and returns according class probabilities from the classifier as numpy arrays\n",
    " 2. Initialize DeepView-object and pass the created method to the constructor\n",
    " 3. Run your code and call ```add_samples(samples, labels)``` at any time to add samples to the visualization together with the ground truth labels.\n",
    "    * The ground truth labels will be visualized along with the predicted labels\n",
    "    * The object will keep track of a maximum number of samples specified by ```max_samples``` and it will throw away the oldest samples first\n",
    " 4. Call the ```show``` method to render the plot\n",
    "\n",
    "The following parameters must be specified on initialization:\n",
    "\n",
    "| <p align=\"left\">Variable               | <p align=\"left\">Meaning           |\n",
    "|------------------------|-------------------|\n",
    "| <p align=\"left\">(!)```pred_wrapper```     | <p align=\"left\">Wrapper function allowing DeepView to use your model. Expects a single argument, which should be a batch of samples to classify. Returns (valid / softmaxed) prediction probabilities for this batch of samples. |\n",
    "| <p align=\"left\">(!)```classes```          | <p align=\"left\">Names of all different classes in the data. |\n",
    "| <p align=\"left\">(!)```max_samples```      | <p align=\"left\">The maximum amount of samples that DeepView will keep track of. When more samples are added, the oldest samples are removed from DeepView. |\n",
    "| <p align=\"left\">(!)```batch_size```       | <p align=\"left\">The batch size used for classification |\n",
    "| <p align=\"left\">(!)```data_shape```       | <p align=\"left\">Shape of the input data (complete shape; excluding the batch dimension) |\n",
    "| <p align=\"left\">```resolution```       | <p align=\"left\">x- and y- Resolution of the decision boundary plot. A high resolution will compute significantly longer than a lower resolution, as every point must be classified, default 100. |\n",
    "| <p align=\"left\">```cmap```             | <p align=\"left\">Name of the colormap that should be used in the plots, default 'tab10'. |\n",
    "| <p align=\"left\">```interactive```      | <p align=\"left\">When ```interactive``` is True, this method is non-blocking to allow plot updates. When ```interactive``` is False, this method is blocking to prevent termination of python scripts, default True. |\n",
    "| <p align=\"left\">```title```            | <p align=\"left\">Title of the deepview-plot. |\n",
    "| <p align=\"left\">```data_viz```         | <p align=\"left\">DeepView has a reactive plot, that responds to mouse clicks and shows the according data sample, when it is clicked. You can pass a custom visualization function, if ```data_viz``` is None, DeepView will try to show each sample as an image, if possible. (optional, default None)  |\n",
    "| <p align=\"left\">```mapper```           | <p align=\"left\">An object that maps samples from the data space to 2D space. Normally UMAP is used for this, but you can pass a custom mapper as well. (optional)  |\n",
    "| <p align=\"left\">```inv_mapper```       | <p align=\"left\">An object that maps samples from the 2D space to the data space. Normally ```deepview.embeddings.InvMapper``` is used for this, but you can pass a custom inverse mapper as well. (optional)  |\n",
    "| <p align=\"left\">```kwargs```       | <p align=\"left\">Configuration for the embeddings in case they are not specifically given in mapper and inv_mapper. Defaults to ```deepview.config.py```.  (optional)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Torch model\n",
    "\n",
    "1. Initialize a pretrained torch model\n",
    "2. Create CIFAR10 dataset\n",
    "3. Write the wrapper function ```pred_wrapper```\n",
    "    1. (optional) Create a visualization function if you want to visualize single examples by clicking in the DeepView plot.\n",
    "4. Initialize a ```DeepView``` object\n",
    "5. Add samples to DeepView and call ```deepview.show```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created PyTorch model:\t ResNet\n",
      " * Dataset:\t\t CIFAR10\n",
      " * Best Test prec:\t 91.78000183105469\n",
      "Files already downloaded and verified\n",
      "\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# device will be detected automatically\n",
    "# Set to 'cpu' or 'cuda:0' to set the device manually\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# get torch model\n",
    "torch_model = demo.create_torch_model(device)\n",
    "# get CIFAR-10 data\n",
    "testset = demo.make_cifar_dataset()\n",
    "\n",
    "print('\\nUsing device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax operation to use in pred_wrapper\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "# this is the prediction wrapper, it encapsulates the call to the model\n",
    "# and does all the casting to the appropriate datatypes\n",
    "def pred_wrapper(x):\n",
    "    with torch.no_grad():\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        tensor = torch.from_numpy(x).to(device)\n",
    "        logits = torch_model(tensor)\n",
    "        probabilities = softmax(logits).cpu().numpy()\n",
    "    return probabilities\n",
    "\n",
    "def visualization(image, point2d, pred, label=None, title=None):\n",
    "    f, a = plt.subplots()\n",
    "    a.set_title(title)\n",
    "    a.imshow(image.transpose([1, 2, 0]))\n",
    "\n",
    "# the classes in the dataset to be used as labels in the plots\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# --- Deep View Parameters ----\n",
    "batch_size = 512\n",
    "max_samples = 500\n",
    "data_shape = (3, 32, 32)\n",
    "n = 5\n",
    "lam = .0\n",
    "resolution = 100\n",
    "cmap = 'tab10'\n",
    "title = 'ResNet-20 - CIFAR10'\n",
    "\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    data_shape, n, lam, resolution, cmap, title=title, data_viz=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n",
      "Computing decision regions ...\n",
      "Time to calculate visualization for 200 samples: 44.74 sec\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(testset), n_samples)\n",
    "X = np.array([ testset[i][0].numpy() for i in sample_ids ])\n",
    "Y = np.array([ testset[i][1] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The visualization contains mostly collapsed clusters of data samples, because $\\lambda$ isn't configured, yet.\n",
    "\n",
    "\n",
    "## Tuning the $\\lambda$ Hyperparameter\n",
    "\n",
    "For $\\lambda = 0$, the DeepView plot will be organized according to the certainty of the model. When this is used with a perfect model, the embedding will contain collapsed class clusters, as equally predicted points will be mapped to the same position.\n",
    "\n",
    "<img alt=\"img_collapsed_clusters\" width=400px src=\"https://user-images.githubusercontent.com/30961397/90417945-be51fa00-e0b4-11ea-8951-d0183432c90b.png\">\n",
    "\n",
    "For $\\lambda = 1$, the DeepView plot will be organized according to structural properties of the datapoints, ignoring the predictions of the model. So for most image-datasets, the points will be scattered all over the place, as the structure itself is an insufficient indicator for sample class.\n",
    "\n",
    "<img alt=\"img_diffused_clusters\" width=400px src=\"https://user-images.githubusercontent.com/30961397/90418110-fc4f1e00-e0b4-11ea-8c85-fa3b53a8e531.png\">\n",
    "\n",
    "A balance must be found here in order to get class clusters that are organized by the structural properties of the data points.\n",
    "\n",
    "**The [DeepView-paper](https://www.ijcai.org/Proceedings/2020/0319.pdf) suggests the following tuning method for $\\lambda$:**\n",
    "\n",
    "As a metric for how well the embedding $\\pi$ is representing the view of the model, the consistency of the point clusters in the embedding with the models predictions can be used:\n",
    "> [...] points that are close to each other should be points that are classified similarity by the classification model.\n",
    "\n",
    "To evaluate on this, a KNN model is trained on the embedded samples with the model predictions as labels.\n",
    "Thus, the leave-one-out error $Q_{kNN}$ of this classifier is the according metric. In order to tune $\\lambda$,\n",
    "\n",
    "> [...] we evaluate $\\pi$ for $Q_{kNN}$ with $\\lambda \\in [0.2;0.8]$ and choose the largest one that does not degrade $Q_{kNN}$ significantly.[...]\n",
    "\n",
    "This is applied here for 6 values linearly distributed accross $[0; 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of DeepView: ResNet-20 - CIFAR10\n",
      "\n",
      "Lambda: 0.00 - Q_kNN: 0.025\n",
      "Lambda: 0.20 - Q_kNN: 0.020\n",
      "Lambda: 0.40 - Q_kNN: 0.035\n",
      "Lambda: 0.60 - Q_kNN: 0.065\n",
      "Lambda: 0.80 - Q_kNN: 0.570\n",
      "Lambda: 1.00 - Q_kNN: 0.825\n"
     ]
    }
   ],
   "source": [
    "from deepview.evaluate import evaluate_umap\n",
    "\n",
    "print('Evaluation of DeepView: %s\\n' % deepview.title)\n",
    "\n",
    "for l in np.linspace(0., 1., 6):\n",
    "    deepview.verbose = False\n",
    "    deepview.set_lambda(l)\n",
    "    q_knn = evaluate_umap(deepview, X, Y, True)['pred']['fish_umap']\n",
    "    print('Lambda: %.2f - Q_kNN: %.3f' % (l, q_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.60 - Q_kNN: 0.065\n",
      "Lambda: 0.64 - Q_kNN: 0.070\n",
      "Lambda: 0.68 - Q_kNN: 0.135\n",
      "Lambda: 0.72 - Q_kNN: 0.245\n",
      "Lambda: 0.76 - Q_kNN: 0.355\n",
      "Lambda: 0.80 - Q_kNN: 0.570\n"
     ]
    }
   ],
   "source": [
    "for l in np.linspace(0.6, 0.8, 6):\n",
    "    deepview.verbose = False\n",
    "    deepview.set_lambda(l)\n",
    "    q_knn = evaluate_umap(deepview, X, Y, True)['pred']['fish_umap']\n",
    "    print('Lambda: %.2f - Q_kNN: %.3f' % (l, q_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "These values show, that for $\\lambda \\lesssim 0.64$, classification clusters are developing in the embedding. For a greater $\\lambda$, the clusters are diffusing too much. So choosing $\\lambda = 0.65$ will allow the clusters to form, while also letting the structural information shape the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.set_lambda(0.65)\n",
    "deepview.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot after $\\lambda$-tuning**\n",
    "\n",
    "<img alt=\"img_good_clusters\" width=500px src=\"https://user-images.githubusercontent.com/30961397/90418547-8a2b0900-e0b5-11ea-8ae2-2b4a76c2e6ce.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
